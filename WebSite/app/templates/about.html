{% extends "base.html" %}

{% block content %}
<section id="box">
<div class="row">

    <div class="col-sm-1"></div>

    <div class="col-md-8">

        <div class="page-header">
            <h1>About the school of computer sarcasm</h1>
        </div>

        <p style="font-family:'Comic Sans MS'; font-weight: bold;">
        This website has been made to show the capabilities of computer intelligence in sarcasm analysis.
        </p>

        <h4>What is sarcasm anyway?</h4>

        <p>Sarcasm is defined in the Cambridge dictionary as "the use of remarks that clearly mean the opposite of what they say, made in order to hurt
            someone's feelings or to criticize something in a humorous way"; or if you happen to be American,
        "remarks that mean the opposite of what they say, made to criticize someone or something in a way that is amusing
            to others but annoying to the person criticized".</p>

        <h4>So why is it hard to detect?</h4>

        <p>Sarcasm analysis is a complex task for humans. Often in conversation sarcasm is noticed through the use of intonation
            or body language. In a written medium these cues are not available, and humans will often rely on contextual information.
            If we read the following sentence "Yea, great!!!!!" do we know if it is sarcastic? This could mean that someone is extremely excited
            about something, or it could mean that they are being very sarcastic, in isolation it is extremely hard to tell.<br>
            In online communication the problem is compounded as people place short comments in isolation, such as below an article on
            Facebook. Unless the person reading it also sees the article they may not be able to judge the sarcasm, and even then
            it is not always clear due to vastly differing opinions.
        </p>
        <h4>Okay... so how does a computer program begin to understand this</h4>
        <p>
            This is where the Machine Learning topic of Natural Language Processing comes in.
        </p>
        <h4>Yes.... Natural Language Processing....I know exactly what you are talking about</h4>
        <p>
            Natural Language Processing is an area of computer science whereby computers are programmed to process analyse large amounts of natural
            language data. This information is then used by the computer to perform a desired function. <br>
            In slightly simpler terms this means that we take a computer program and show it a lot of text with some additional
             information (e.g. whether or not it is sarcastic) and then apply this to a desired use, such as to tell us if
            a new sentence it has never seen before is sarcastic or not.
        </p>
        <h4>This sounds quite simple, why is everyone not doing it?</h4>
        <p>
        Well in fact lot's of applications do use it! Google translate is entirely based on this, when you use a chatbot
            on Facebook to find out information from a company, any sort of speech to text (e.g. Siri) is based on this as well.
            <br> But, it isn't as simple as it sounds.
        </p>
        <h4>So you have to do more than simply show a program a lot of text?</h4>
        <p>
            You do indeed! The raw data you show the program has to go through several processes, the first of which is called
            "preprocessing". This involves taking the raw textual input and removing parts which you do not think are useful. <br>
            In this application we remove what are called "stopwords". These are extremely common words which do not add much
            analytical value to the sentence, such as "the" "that" or "a". We then remove all sentences which are less than 3
            words long from the dataset as these carry little analytical value.
        </p>
        <h4>And then it's ready to use?</h4>
        <p>
            At this point the sentence has passed our criteria to be included in the dataset, but we now need to extract the useful
            information from it; this information is called the "features".
        </p>
        <h4>Why do we need to take out more information?</h4>
        <p>
            Computers work best on numerical information. If we can assign a numerical value to information which is related to
            what we are trying to find then we can teach the computer the patterns to look for so it can find it too!<br>
            We therefore need to see patterns, separate them from the rest of the data and find a way to represent them for the computer.
        </p>
        <h4>This is sounding like a lot of work to be honest...</h4>
        <p>
            It may be, but the rewards are huge. If we can find meaningful patterns and extract the information then we can utilise a computer's
            processing power to free up humans for work which requires our skills, or to aid us in fulfilling tasks. <br>
            Take the example of Google Translate. By teaching this program how to translate from one language to another we have been
            able to go all across the world and communicate with other's without the need to know the language ourselves. Training
            that application has then aided millions of people going about their lives and also saved them much time.<br>
            Another very common application is the Spam filter on your email. Through learning which emails you read and reply to,
            and given the Spam selection you apply, and others also apply, your email will only display relevant emails to you.
        </p>
        <h4>Okay, so it is very useful! But what features indicate sarcasm?</h4>
        <p>
            As this application seeks to look at a sentence in isolation it has no context to learn from. Therefore it needs to
            be given clues from the sentence alone. A study by Riloff et al. (2014) suggested a model where the sentiment of a sentence
            (the positivity or negativity in it) would be opposites in different parts of the sentence. For example, consider the following
            "I love it when my car breaks down and I am late". "I love it" is positive, but "when my car breaks down and I am late" is negative.
            <br>To find these subtleties we can use sentiment analysis tools to find the sentiment score for different parts of
            a sentence (each third and both halves) and then look for contrast between these scores. This contrast gives us a
            numerical value to pass to our training model.
            <br>We also program the computer to count if there are more than 4 capital letters, how many exclamation and sentence marks
            are present and the number of happy and sad emoticons used. The number of nouns, verbs, adverbs and adjectives are counted
            and all sequences of two words, known as bigrams, are saved.<br>
            Finally topics, a way of analysing bodies of texts to remove words used together which are then called a "topic".
            If these sequences are found to frequently occur in either sarcastic or non-sarcastic texts then it aids the computer
            in finding this.
        </p>
        <h4>And what text is actually used for this?</h4>
        <p>
            Due to the time consuming nature of reading and annotating thousands of text files a set of 1.5 million reddit comments,
            pre annotated as sarcastic or not, was sourced. The sarcastic comments were labelled as such due to their including
            the tag "sarcasm" on reddit. Although this is a body of texts from a specific social media medium it's applicability should
            extend beyond Reddit comments and apply to other short pieces of text.
        </p>
        <h4>So you preprocess, you extract features and then...?</h4>
        <p>
            You show the features to a model, which is a computer algorithm that can spot patterns in data and use it to make
            predictions with a certain degree of accuracy. For this project a Support Vector Machine was chosen as it is commonly
            used in NLP and performed well in the selection tests performed.
        </p>
        <h4>And then it's ready to use?</h4>
        <p>
            Yes! Then it is ready to use; however, the more data the model is shown the better it performs. To show this the option
            to change the dataset size and see how this changes your result has been included. <br>
        </p>
        <h4>Play around with the Sarcasm Score Generator, try lots of sentences and dataset sizes and see how it changes results!<br>
            Hopefully you now know a little bit more about the world of Machine Learning and Natural Language processing!</h4>
    </div>
</div>
    </section>
{% endblock %}